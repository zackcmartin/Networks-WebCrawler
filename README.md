Our high level approach for this project was to first appropriately handle the input username and password from the command line. This code we took pretty much from our previous assignments handling input. We then needed to handle logging in to fakebook, so we opened a socket and sent a GET HTTP request to the fakebook login url. The response, when received, we fed to our HTML parser which we were able to use to get the cookie to use to log in. We then sent the appropriate POST message with the log in credentials we got from the command line (we found this POST investigating the page on Firefox). We then looked at the response to retrieve the CSRF token in order to remain logged in. After this, we were good to scrape and began a loop that would not stop until we either had all 5 secret tokens or we ran out of URLs to scrape. We had two lists, one with URLs we’ve seen and one with URLs we still need to check. We would send a get request for each URL, and then feed the response to our HTML parser, where any headers with “a” (and start with a “/“) we would add that link to the list of URLs to check if we haven’t already seen it. Additionally, if there was any data in the page with a FLAG:, we would save the secret flag to our secret flag list. For each response, we would check the status code and behave appropriately depending on what the status code was. If one of the responses had a connection: close, we would open  a new socket and reconnect. Lastly, in order to handle chunks appropriately, if there was an <html> tag in the response without a </html> tag, we would loop through and continue to receive data until we found it. 
This was actually one of the most challenging parts of our project, because we would loop for some time and then end up receiving no response from the server. We tried a timer, which didn’t help. What fixed it was adding this “<html>” check, instead of the loop we had before. This took us quite some time to figure out. Getting a handle of how the log in worked was also quite challenging, but once we got the hang of it the project started making more sense.
To test, we had various print statements to check the data as we would loop through and scrape the pages. This data helped a lot with errors we would get, such as a response having connection: close without us handling that case. It also helped when we had trouble with the batches of responses, because we were able to see where the sever responses were not being sent out together. 
